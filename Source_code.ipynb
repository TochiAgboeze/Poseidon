{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# # Directory to Raw ground water levels folder\n",
        "#The Grundwasserstand-Monatsmittel (monthly average groundwater level) directory is contained in the ehyd_messstellen_all_gw.zip file\n",
        "directory = \"/content/Grundwasserstand-Monatsmittel\" # Adjust to corresponding folder on your device\n",
        "\n",
        "# Folder Path for cleaned sampling points\n",
        "cleaned_output_folder = 'sampling_points_cleaned'\n",
        "\n",
        "# Folder path for the 487 sample points needed for prediction\n",
        "folder_path_487 = \"filtered/\""
      ],
      "metadata": {
        "id": "3nCQpp9zStvf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9MEwPmpMsXj5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985da8e7-8398-44a6-ac2d-e1556b72883c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from lightgbm import LGBMRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NMkSXmVipbXi"
      },
      "outputs": [],
      "source": [
        "# Function to get SMAPE score\n",
        "def smape(A, F):\n",
        "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning Process**"
      ],
      "metadata": {
        "id": "e_tr2G7VHHvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the raw files\n",
        "import os\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".csv\") and filename.startswith(\"Grundwasserstand-Monatsmittel-\"):\n",
        "        new_filename = filename.replace(\"Grundwasserstand-Monatsmittel-\", \"\")\n",
        "        old_filepath = os.path.join(directory, filename)\n",
        "        new_filepath = os.path.join(directory, new_filename)\n",
        "        os.rename(old_filepath, new_filepath)"
      ],
      "metadata": {
        "id": "Lqi_N_inN4UU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ENCODING and cleaning the raw files to have just the metadata (months and their groundwater levels)\n",
        "\n",
        "import os\n",
        "\n",
        "def process_file(input_file, output_file, encoding='ISO-8859-1'):\n",
        "    # Read the input file with a specific encoding\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding=encoding) as f:\n",
        "            lines = f.readlines()\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Error decoding {input_file} with encoding {encoding}.\")\n",
        "        return\n",
        "\n",
        "    # Find the line starting with 'Werte:'\n",
        "    start_index = None\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.startswith('Werte:'):\n",
        "            start_index = i\n",
        "            break\n",
        "\n",
        "    if start_index is None:\n",
        "        print(f\"Warning: 'Werte:' not found in {input_file}.\")\n",
        "        return\n",
        "\n",
        "    relevant_lines = lines[start_index + 1:]\n",
        "\n",
        "    with open(output_file, 'w', encoding=encoding) as f:\n",
        "        f.writelines(relevant_lines)\n",
        "\n",
        "    print(f\"Processed {input_file} and saved to {output_file}.\")\n",
        "\n",
        "def process_folder(folder_path, output_folder, encoding='ISO-8859-1'):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.csv'):\n",
        "            input_file = os.path.join(folder_path, filename)\n",
        "            output_file = os.path.join(output_folder, filename)\n",
        "            process_file(input_file, output_file, encoding)\n",
        "\n",
        "\n",
        "process_folder(directory, cleaned_output_folder)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SS2p2_43N3Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning data format\n",
        "\n",
        "for i in os.listdir(cleaned_output_folder):\n",
        "  try:\n",
        "    path=os.path.join(cleaned_output_folder, i)\n",
        "    df=pd.read_csv(path, encoding='latin-1', header=None)\n",
        "    df[\"data\"]=df[0].str.split(';')\n",
        "    df_=pd.DataFrame(df[\"data\"].to_list(), columns=['Date', 'Data1', 'Data2'])\n",
        "    df[1]=df[1].str.strip(' ;')\n",
        "    df_[1]=df[1]\n",
        "    df_[\"Levels\"]=df_[\"Data1\"]+\".\"+df_[1]\n",
        "    df_=df_[[\"Date\",\"Levels\"]]\n",
        "    df_[\"Levels\"]=df_[\"Levels\"].astype(\"float64\")\n",
        "    df_.to_csv(path, index=False)\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "Z9UwdbBVHK1l",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 487 points needed for predictions\n",
        "\n",
        "points_data_487= ['330803.csv', '305714.csv', '335562.csv', '345330.csv', '301572.csv', '318584.csv', '307843.csv',\n",
        " '331116.csv', '329995.csv', '315671.csv', '308783.csv', '335984.csv', '306209.csv', '316026.csv',\n",
        " '334052.csv', '320754.csv', '326132.csv', '329573.csv', '322156.csv', '328724.csv', '345132.csv',\n",
        " '322396.csv', '325969.csv', '312447.csv', '332783.csv', '300269.csv', '300137.csv', '324095.csv',\n",
        " '304741.csv', '335588.csv', '327239.csv', '345694.csv', '327171.csv', '306183.csv', '312900.csv',\n",
        " '311548.csv', '345496.csv', '319921.csv', '335372.csv', '317461.csv', '319699.csv', '321448.csv',\n",
        " '311951.csv', '345454.csv', '314021.csv', '322578.csv', '304352.csv', '335992.csv', '330829.csv',\n",
        " '335414.csv', '345124.csv', '345520.csv', '311944.csv', '328690.csv', '332569.csv', '330381.csv',\n",
        " '345736.csv', '316356.csv', '327809.csv', '331058.csv', '335216.csv', '307298.csv', '324327.csv',\n",
        " '374074.csv', '305896.csv', '330811.csv', '331272.csv', '345322.csv', '328864.csv', '345256.csv',\n",
        " '305672.csv', '335570.csv', '305706.csv', '335612.csv', '305060.csv', '326108.csv', '345108.csv',\n",
        " '335174.csv', '305920.csv', '327411.csv', '312736.csv', '302901.csv', '320747.csv', '345652.csv',\n",
        " '345280.csv', '329789.csv', '321471.csv', '305854.csv', '331298.csv', '326280.csv', '315689.csv',\n",
        " '345041.csv', '303982.csv', '303016.csv', '326082.csv', '335968.csv', '313239.csv', '327163.csv',\n",
        " '315853.csv', '335836.csv', '310268.csv', '323675.csv', '345447.csv', '310532.csv', '330027.csv',\n",
        " '374314.csv', '310862.csv', '345686.csv', '335349.csv', '302240.csv', '326447.csv', '325167.csv',\n",
        " '328443.csv', '313833.csv', '345645.csv', '308585.csv', '333088.csv', '305102.csv', '345678.csv',\n",
        " '376517.csv', '335638.csv', '335604.csv', '376715.csv', '335943.csv', '311639.csv', '318345.csv',\n",
        " '318444.csv', '319772.csv', '336008.csv', '319202.csv', '345025.csv', '321992.csv',\n",
        " '304535.csv', '335844.csv', '314054.csv', '309005.csv', '345421.csv', '328815.csv', '305987.csv', '311845.csv', '308247.csv', '311266.csv', '330928.csv',\n",
        " '328419.csv', '322479.csv', '331439.csv', '324020.csv', '309617.csv', '335893.csv', '323410.csv', '335851.csv', '328021.csv',\n",
        " '321950.csv', '306928.csv', '335448.csv', '323766.csv', '314294.csv', '335299.csv', '345595.csv', '376954.csv', '300236.csv', '326595.csv', '316091.csv', '335927.csv', '303503.csv', '305953.csv', '301937.csv', '308668.csv', '325894.csv', '306043.csv', '322255.csv', '335729.csv',\n",
        " '305946.csv', '335661.csv', '335885.csv', '325738.csv', '325274.csv', '319764.csv', '326231.csv', '335067.csv', '321984.csv', '318485.csv', '306456.csv', '329847.csv',\n",
        " '309211.csv', '301846.csv', '323832.csv', '319947.csv', '330480.csv', '345355.csv', '303248.csv', '305995.csv', '306522.csv', '345553.csv', '335117.csv', '345157.csv', '318873.csv', '335315.csv', '326355.csv', '323204.csv', '313304.csv', '345397.csv', '309823.csv', '345181.csv',\n",
        "'306084.csv', '323774.csv', '313338.csv', '301648.csv', '313700.csv', '330456.csv', '345744.csv', '326181.csv', '329649.csv', '321836.csv', '314641.csv', '313064.csv', '345546.csv', '302380.csv', '305821.csv', '345587.csv', '335869.csv', '323428.csv', '326140.csv', '327114.csv', '326975.csv', '303263.csv', '328435.csv', '306092.csv', '300384.csv', '335471.csv', '304691.csv', '345140.csv', '316083.csv', '319830.csv', '313460.csv', '345579.csv', '303909.csv', '324434.csv', '335539.csv', '300780.csv', '300970.csv', '326223.csv', '345223.csv', '314534.csv', '304733.csv', '323550.csv', '330910.csv', '335497.csv', '300996.csv', '335695.csv', '305540.csv', '338616.csv', '325928.csv', '323154.csv', '328773.csv', '345603.csv', '325134.csv', '335455.csv', '331223.csv', '308924.csv', '315960.csv', '302992.csv', '345165.csv', '379313.csv', '319418.csv', '322115.csv', '312611.csv', '345561.csv', '319236.csv', '321554.csv', '335737.csv', '335521.csv', '345207.csv', '315168.csv', '316661.csv', '304071.csv', '375113.csv', '317594.csv', '301309.csv', '346056.csv', '309609.csv', '345371.csv', '319426.csv', '307793.csv', '345629.csv', '300822.csv', '305755.csv', '329268.csv', '306274.csv', '345173.csv', '304675.csv', '330274.csv', '335331.csv', '307355.csv', '326371.csv', '302588.csv', '307397.csv', '327536.csv', '316265.csv', '345199.csv', '330738.csv', '374967.csv', '331397.csv', '326199.csv', '323618.csv', '335091.csv', '319962.csv', '317230.csv', '305813.csv', '328211.csv', '307157.csv', '345348.csv', '335877.csv', '313668.csv', '315580.csv', '310995.csv', '307769.csv', '304923.csv', '375923.csv', '321778.csv', '326413.csv', '376608.csv', '309427.csv', '335679.csv', '317396.csv', '309625.csv', '345413.csv', '304063.csv', '330852.csv', '345017.csv', '345215.csv', '301127.csv', '303917.csv', '312165.csv', '309021.csv', '309948.csv', '335109.csv', '319434.csv', '345363.csv', '314104.csv', '345405.csv', '345439.csv', '306266.csv', '313643.csv', '321752.csv', '309419.csv', '326439.csv', '324038.csv', '321430.csv', '322313.csv', '335323.csv', '316505.csv', '335653.csv', '300616.csv', '329078.csv', '326389.csv', '301440.csv', '345389.csv', '328401.csv', '310607.csv', '377887.csv', '335646.csv', '300400.csv', '315390.csv', '300665.csv', '310029.csv', '322925.csv', '335901.csv', '303727.csv', '309906.csv', '328260.csv', '309641.csv', '327437.csv', '319541.csv', '345488.csv', '322610.csv', '309872.csv', '335422.csv', '325142.csv', '329037.csv', '326462.csv', '319889.csv', '305292.csv', '345728.csv', '326264.csv', '335620.csv', '304600.csv', '313817.csv', '345462.csv', '345660.csv', '345264.csv', '335026.csv', '345272.csv', '345058.csv', '313544.csv', '335554.csv', '307082.csv', '345066.csv', '329169.csv', '302307.csv', '306613.csv', '326306.csv', '314161.csv', '323295.csv', '312504.csv', '304170.csv', '323097.csv', '306001.csv', '335018.csv', '345298.csv', '304428.csv', '327031.csv', '328666.csv', '305904.csv', '323055.csv', '328302.csv', '330001.csv', '335810.csv', '305938.csv', '323121.csv', '307520.csv', '326298.csv', '335596.csv', '301838.csv', '331082.csv', '329144.csv', '313569.csv', '300111.csv', '326934.csv', '335182.csv', '327551.csv', '311381.csv', '307124.csv', '306399.csv', '335828.csv', '331124.csv', '328104.csv', '326868.csv', '310672.csv', '312918.csv', '335430.csv', '323253.csv', '335141.csv', '305524.csv', '321646.csv', '376657.csv', '317487.csv', '345504.csv', '318824.csv', '309054.csv', '316000.csv', '345470.csv', '326074.csv', '304410.csv', '306167.csv', '306415.csv', '316174.csv', '325019.csv', '316612.csv', '326504.csv', '312660.csv', '345538.csv', '335208.csv', '317446.csv', '345512.csv', '326843.csv', '335778.csv', '322990.csv', '345710.csv', '345116.csv', '323709.csv',\n",
        "  '331330.csv', '319053.csv', '374678.csv', '319442.csv', '345314.csv', '313387.csv', '305268.csv', '305862.csv', '304956.csv', '326249.csv', '329813.csv', '301812.csv', '335976.csv', '345249.csv', '327619.csv', '335547.csv']"
      ],
      "metadata": {
        "id": "MAFbN3AUhySQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy to a new folder 487 sampling points\n",
        "\n",
        "os.mkdir(folder_path_487)\n",
        "\n",
        "for i in points_data_487:\n",
        "  old_path=os.path.join(cleaned_output_folder, i)\n",
        "  new_path=os.path.join(folder_path_487, i)\n",
        "  shutil.copyfile(old_path, new_path)"
      ],
      "metadata": {
        "id": "sA26v39Ih_KT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dl-KrrexDLk"
      },
      "source": [
        "## **Feature Extraction Technique with LightGBM Model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Model\n",
        "model_name=\"LightGBM\"\n",
        "model=LGBMRegressor(verbose=-1)\n",
        "\n",
        "# Create empty lists\n",
        "Pred=[]\n",
        "actuals=[]\n",
        "\n",
        "# Loop through each csv file\n",
        "for i in os.listdir(folder_path_487):\n",
        "\n",
        "  # Read the csv file\n",
        "  path=os.path.join(folder_path_487, i)\n",
        "  df=pd.read_csv(path)\n",
        "\n",
        "  # Remove last row and fill na's\n",
        "  df=df.iloc[:-1,:]\n",
        "  df.ffill(inplace=True)\n",
        "\n",
        "  # Convert date to datetime feature\n",
        "  df[\"Date\"]=pd.to_datetime(df[\"Date\"], format=\"%d.%m.%Y %H:%M:%S   \")\n",
        "  df.sort_values(by=\"Date\", inplace=True)\n",
        "\n",
        "  # Extract datetime features\n",
        "  df[\"Day\"]=df[\"Date\"].dt.day\n",
        "  df[\"Dayofweek\"]=df[\"Date\"].dt.dayofweek\n",
        "  df[\"Month\"]=df[\"Date\"].dt.month\n",
        "  df[\"Year\"]=df[\"Date\"].dt.year\n",
        "  df[\"Quarter\"]=df[\"Date\"].dt.quarter\n",
        "  df[\"Week\"]=df[\"Date\"].dt.isocalendar().week\n",
        "  df=df.drop(\"Date\", axis=1)\n",
        "\n",
        "  # Split data to train and test\n",
        "  train=df.iloc[:-26,:]\n",
        "  test=df.iloc[-26:, :]\n",
        "\n",
        "  # Split data to dependent and independent variables\n",
        "  X_train=train.drop(\"Levels\", axis=1)\n",
        "  y_train=train[\"Levels\"]\n",
        "  X_test=test.drop(\"Levels\", axis=1)\n",
        "  y_test=test[\"Levels\"]\n",
        "\n",
        "  # Train model\n",
        "  model.fit(X_train, y_train)\n",
        "  pred=model.predict(X_test)\n",
        "\n",
        "  # Test the model\n",
        "  for yss in y_test.values:\n",
        "    actuals.append(yss)\n",
        "  for yss in pred:\n",
        "    Pred.append(yss)\n",
        "\n",
        "# Create dataframe for y_tests and corresponding Predictions\n",
        "test=pd.DataFrame({\"Predictions\":Pred, \"Actuals\":actuals}).dropna()\n",
        "\n",
        "# Calculate SMAPE Score\n",
        "A = test[\"Actuals\"]\n",
        "F = test[\"Predictions\"]\n",
        "print(model_name+\" SMAPE Score: \", smape(A, F))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7-bsKWXG5dl",
        "outputId": "f8b99420-60b4-44dd-f6ff-4846e33112a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM SMAPE Score:  0.12082219218344961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Forecast for 2022 - 2024 Feb**"
      ],
      "metadata": {
        "id": "oO4x7KzCI_hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a date range starting from 01.01.2022 with monthly frequency\n",
        "date_range = pd.date_range(start='2022-01-01', end='2024-02-01', freq='MS')\n",
        "\n",
        "# Create a DataFrame\n",
        "df_ = pd.DataFrame(date_range, columns=['Date'])\n",
        "\n",
        "# Extract Features\n",
        "df_[\"Day\"]=df_[\"Date\"].dt.day\n",
        "df_[\"Dayofweek\"]=df_[\"Date\"].dt.dayofweek\n",
        "df_[\"Month\"]=df_[\"Date\"].dt.month\n",
        "df_[\"Year\"]=df_[\"Date\"].dt.year\n",
        "df_[\"Quarter\"]=df_[\"Date\"].dt.quarter\n",
        "df_[\"Week\"]=df_[\"Date\"].dt.isocalendar().week\n",
        "\n",
        "df_.drop(\"Date\", axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "XPi57SVzJJEE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Pred=[]\n",
        "\n",
        "# Loop through each CSV file\n",
        "for i in os.listdir(folder_path_487):\n",
        "\n",
        "    # Read CSV, drop 2022 and fill na\n",
        "    path=os.path.join(folder_path_487, i)\n",
        "    df=pd.read_csv(path)\n",
        "    df=df.iloc[:-1,:]\n",
        "    df.ffill(inplace=True)\n",
        "\n",
        "    # Change Date column to datetime features\n",
        "    df[\"Date\"]=pd.to_datetime(df[\"Date\"], format=\"%d.%m.%Y %H:%M:%S   \")\n",
        "    df.sort_values(by=\"Date\", inplace=True)\n",
        "\n",
        "    # Extract Date features\n",
        "    df[\"Day\"]=df[\"Date\"].dt.day\n",
        "    df[\"Dayofweek\"]=df[\"Date\"].dt.dayofweek\n",
        "    df[\"Month\"]=df[\"Date\"].dt.month\n",
        "    df[\"Year\"]=df[\"Date\"].dt.year\n",
        "    df[\"Quarter\"]=df[\"Date\"].dt.quarter\n",
        "    df[\"Week\"]=df[\"Date\"].dt.isocalendar().week\n",
        "    df=df.drop(\"Date\", axis=1)\n",
        "\n",
        "    # Split to dependent and independent variables\n",
        "    X=df.drop(\"Levels\", axis=1)\n",
        "    y=df[\"Levels\"]\n",
        "\n",
        "    # Train model\n",
        "    model=LGBMRegressor(verbose=-1)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Make forecast\n",
        "    pred=model.predict(df_)\n",
        "\n",
        "    # Append forecast and site id to Pred list\n",
        "    Pred.append([i.split(\".\")[0], pred])\n",
        "\n",
        "# Generate dataframe for forecasted data\n",
        "preds=pd.DataFrame(Pred)\n",
        "sites=preds[0].values\n",
        "preds=pd.DataFrame(preds[1].to_list())\n",
        "preds['Sites']=sites\n",
        "preds.set_index('Sites', inplace=True)\n",
        "preds=preds.T\n",
        "preds[\"Date\"] = date_range\n",
        "preds.set_index(\"Date\", inplace=True)\n",
        "preds=preds.round(2)\n",
        "preds\n",
        "preds.to_csv(\"groundwater_forecasts.csv\")"
      ],
      "metadata": {
        "id": "1gRUS9c_JRfw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train and Make Prediction For Specific Geographic Location**"
      ],
      "metadata": {
        "id": "HsbXBNgJG6hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path for the training data\n",
        "file_path = \"/content/Grundwasserstand-Monatsmittel/300111.csv\"  # Change to the specific data file path\n",
        "# Define the start and end dates for the forecast\n",
        "start_date = \"2022-01-01\"  # Change to desired start date\n",
        "end_date = \"2024-02-01\"    # Change to desired end date\n"
      ],
      "metadata": {
        "id": "rjm69KDPUsYi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from lightgbm import LGBMRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "_faYGVZXwhmF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(input_file, output_file, encoding='ISO-8859-1'):\n",
        "    # Read the input file with a specific encoding\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding=encoding) as f:\n",
        "            lines = f.readlines()\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Error decoding {input_file} with encoding {encoding}.\")\n",
        "        return\n",
        "\n",
        "    # Find the line starting with 'Werte:'\n",
        "    start_index = None\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.startswith('Werte:'):\n",
        "            start_index = i\n",
        "            break\n",
        "\n",
        "    if start_index is None:\n",
        "        print(f\"Warning: 'Werte:' not found in {input_file}.\")\n",
        "        return\n",
        "\n",
        "    relevant_lines = lines[start_index + 1:]\n",
        "\n",
        "    with open(output_file, 'w', encoding=encoding) as f:\n",
        "        f.writelines(relevant_lines)\n",
        "\n",
        "    print(f\"Processed {input_file} and saved to {output_file}.\")\n",
        "\n",
        "\n",
        "def clean_data(file_path):\n",
        "\n",
        "  file_name=file_path\n",
        "\n",
        "  process_file(file_name, 'cleaned_input.csv', encoding='ISO-8859-1')\n",
        "\n",
        "  df=pd.read_csv(\"cleaned_input.csv\", encoding='latin-1', header=None)\n",
        "  df[\"data\"]=df[0].str.split(';')\n",
        "  df_=pd.DataFrame(df[\"data\"].to_list(), columns=['Date', 'Data1', 'Data2'])\n",
        "  df[1]=df[1].str.strip(' ;')\n",
        "  df_[1]=df[1]\n",
        "  df_[\"Levels\"]=df_[\"Data1\"]+\".\"+df_[1]\n",
        "  df_=df_[[\"Date\",\"Levels\"]]\n",
        "  df_[\"Levels\"]=df_[\"Levels\"].astype(\"float64\")\n",
        "  return df_"
      ],
      "metadata": {
        "id": "mFf9mYTSnN7q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_forecast_dates(start_date = \"2022-01-01\", end_date=\"2024-02-01\"):\n",
        "\n",
        "  # Create a date range starting from start date with monthly frequency\n",
        "  date_range = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
        "\n",
        "  # Create a DataFrame\n",
        "  df_ = pd.DataFrame(date_range, columns=['Date'])\n",
        "\n",
        "  # Extract Date Features from DateFrame\n",
        "  df_[\"Day\"]=df_[\"Date\"].dt.day\n",
        "  df_[\"Dayofweek\"]=df_[\"Date\"].dt.dayofweek\n",
        "  df_[\"Month\"]=df_[\"Date\"].dt.month\n",
        "  df_[\"Year\"]=df_[\"Date\"].dt.year\n",
        "  df_[\"Quarter\"]=df_[\"Date\"].dt.quarter\n",
        "  df_[\"Week\"]=df_[\"Date\"].dt.isocalendar().week\n",
        "\n",
        "  return df_"
      ],
      "metadata": {
        "id": "wgCxhPgUCOml"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nfsrTMKWvXKf"
      },
      "outputs": [],
      "source": [
        "def train_model(data_frame):\n",
        "\n",
        "  # Read in the dataset for specific loactaion\n",
        "  df=data_frame\n",
        "\n",
        "  # Remove 2022 row\n",
        "  df=df.iloc[:-1,:]\n",
        "\n",
        "  # Fill missing values\n",
        "  df.ffill(inplace=True)\n",
        "\n",
        "  # Convert date column to datetime type\n",
        "  df[\"Date\"]=pd.to_datetime(df[\"Date\"], format=\"%d.%m.%Y %H:%M:%S   \")\n",
        "  df.sort_values(by=\"Date\", inplace=True)\n",
        "\n",
        "  # Extract Features\n",
        "  df[\"Day\"]=df[\"Date\"].dt.day\n",
        "  df[\"Dayofweek\"]=df[\"Date\"].dt.dayofweek\n",
        "  df[\"Month\"]=df[\"Date\"].dt.month\n",
        "  df[\"Year\"]=df[\"Date\"].dt.year\n",
        "  df[\"Quarter\"]=df[\"Date\"].dt.quarter\n",
        "  df[\"Week\"]=df[\"Date\"].dt.isocalendar().week\n",
        "\n",
        "  # Split data to dependent and independent variables\n",
        "  df=df.drop(\"Date\", axis=1)\n",
        "  X=df.drop(\"Levels\", axis=1)\n",
        "  y=df[\"Levels\"]\n",
        "\n",
        "  # Train and return the model\n",
        "  model=LGBMRegressor(verbose=-1)\n",
        "  model.fit(X, y)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_forecasts(model, forecast_dates):\n",
        "\n",
        "  forecasts=pd.DataFrame()\n",
        "\n",
        "  # Make Predictions for forecast_dates\n",
        "  forecasts[\"Date\"]= forecast_dates[\"Date\"]\n",
        "  forecast_dates.drop(\"Date\", axis=1, inplace=True)\n",
        "  pred=model.predict(forecast_dates)\n",
        "  forecasts[\"Levels\"]=pred\n",
        "  forecasts=forecasts.round(2)\n",
        "\n",
        "  return forecasts"
      ],
      "metadata": {
        "id": "3rUOHAvCF8zP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate DataFrame for forecast dates\n",
        "forecast_dates = make_forecast_dates(start_date=start_date, end_date=end_date)\n",
        "\n",
        "# Clean dataset\n",
        "df_ = clean_data(file_path)\n",
        "\n",
        "# Train the model using the specified data file\n",
        "model = train_model(df_)\n",
        "\n",
        "# Make forecasts using the trained model and forecast dates\n",
        "forecasts = make_forecasts(model, forecast_dates)\n",
        "\n",
        "# Optionally, print or save the forecasts\n",
        "print(forecasts)\n",
        "forecasts.to_csv(\"forecasts.csv\", index=False)\n",
        "print(\"\\nFORECASTS SAVED SUCCESSFULLY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWq4xOvIC5di",
        "outputId": "f4219482-0cfe-4502-ed9a-8de1312e636b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed /content/Grundwasserstand-Monatsmittel/300111.csv and saved to cleaned_input.csv.\n",
            "         Date  Levels\n",
            "0  2022-01-01  154.02\n",
            "1  2022-02-01  153.76\n",
            "2  2022-03-01  153.77\n",
            "3  2022-04-01  153.75\n",
            "4  2022-05-01  153.75\n",
            "5  2022-06-01  153.86\n",
            "6  2022-07-01  154.04\n",
            "7  2022-08-01  154.06\n",
            "8  2022-09-01  154.04\n",
            "9  2022-10-01  154.02\n",
            "10 2022-11-01  154.03\n",
            "11 2022-12-01  154.03\n",
            "12 2023-01-01  154.05\n",
            "13 2023-02-01  153.76\n",
            "14 2023-03-01  153.77\n",
            "15 2023-04-01  153.75\n",
            "16 2023-05-01  153.81\n",
            "17 2023-06-01  153.86\n",
            "18 2023-07-01  154.04\n",
            "19 2023-08-01  154.05\n",
            "20 2023-09-01  154.03\n",
            "21 2023-10-01  154.03\n",
            "22 2023-11-01  154.03\n",
            "23 2023-12-01  154.01\n",
            "24 2024-01-01  153.68\n",
            "25 2024-02-01  153.76\n",
            "\n",
            "FORECASTS SAVED SUCCESSFULLY\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
